{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7166b15e",
   "metadata": {},
   "source": [
    "#  Swin Transformer Backbone (PyTorch)\n",
    "\n",
    "This notebook implements the Swin Transformer architecture from scratch using PyTorch.  \n",
    "Swin Transformers improve efficiency by using hierarchical feature maps and computing self-attention within local windows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68aee50b",
   "metadata": {},
   "source": [
    "## ðŸ“¦ Import Required Libraries\n",
    "\n",
    "We import standard PyTorch libraries and helper functions from `timm` such as `DropPath` and `trunc_normal_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7bf17dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\.conda\\envs\\genai\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python312\\site-packages\\timm\\models\\layers\\__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366c7737",
   "metadata": {},
   "source": [
    "## ðŸ” Feedforward MLP Block - `Mlp` Class\n",
    "\n",
    "This is the feedforward layer used inside each Swin Transformer block.  \n",
    "It consists of two linear layers separated by GELU activation and followed by dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87d0e988",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, act_layer=nn.GELU, drop=0.0):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, in_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48baef51",
   "metadata": {},
   "source": [
    "## ðŸ§© Window Partitioning Functions\n",
    "\n",
    "These utility functions split the input into local windows (`window_partition`) and merge them back after attention (`window_reverse`).  \n",
    "They help reduce attention complexity by limiting attention to small windows instead of the whole image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcda048d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_partition(x, window_size):\n",
    "    B, H, W, C = x.shape\n",
    "    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5).reshape(-1, window_size, window_size, C)\n",
    "    return x\n",
    "\n",
    "# Reverse window partitioning (used after attention)\n",
    "def window_reverse(windows, window_size, H, W):\n",
    "    B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
    "    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5).reshape(B, H, W, -1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71a4235",
   "metadata": {},
   "source": [
    "## ðŸ‘€ Window-Based Self-Attention - `WindowAttention` Class\n",
    "\n",
    "This class implements multi-head self-attention restricted to non-overlapping windows.  \n",
    "It includes a relative positional encoding and supports optional attention masking for shifted windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9dec1f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowAttention(nn.Module):\n",
    "    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0.0, proj_drop=0.0):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or self.head_dim ** -0.5\n",
    "\n",
    "        # Relative position bias table\n",
    "        self.relative_position_bias_table = nn.Parameter(\n",
    "            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads)\n",
    "        )\n",
    "\n",
    "        # Relative position index\n",
    "        coords_h = torch.arange(window_size[0])\n",
    "        coords_w = torch.arange(window_size[1])\n",
    "        coords = torch.stack(torch.meshgrid([coords_h, coords_w], indexing=\"ij\"))\n",
    "        coords_flatten = coords.flatten(1)\n",
    "\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
    "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n",
    "        relative_coords[:, :, 0] += window_size[0] - 1\n",
    "        relative_coords[:, :, 1] += window_size[1] - 1\n",
    "        relative_coords[:, :, 0] *= 2 * window_size[1] - 1\n",
    "        relative_position_index = relative_coords.sum(-1)\n",
    "\n",
    "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "\n",
    "        # Attention layers\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        B_, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "\n",
    "        relative_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)]\n",
    "        relative_bias = relative_bias.view(N, N, -1).permute(2, 0, 1).unsqueeze(0)\n",
    "        attn = attn + relative_bias\n",
    "\n",
    "        if mask is not None:\n",
    "            num_windows = mask.shape[0]\n",
    "            attn = attn.view(B_ // num_windows, num_windows, self.num_heads, N, N)\n",
    "            attn = attn + mask.unsqueeze(1).unsqueeze(0)\n",
    "            attn = attn.view(-1, self.num_heads, N, N)\n",
    "\n",
    "        attn = self.softmax(attn)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14182fbb",
   "metadata": {},
   "source": [
    "## ðŸ”· Swin Transformer Block - `SwinTransformerBlock` Class\n",
    "\n",
    "Each block applies window attention and an MLP layer.  \n",
    "Odd-numbered blocks use **shifted windows** to enable cross-window connections and better modeling of spatial relations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75523c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinTransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0,\n",
    "                 mlp_ratio=4.0, qkv_bias=True, qk_scale=None, drop=0.0, attn_drop=0.0,\n",
    "                 drop_path=0.0, act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size if min(input_resolution) > window_size else 0\n",
    "\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = WindowAttention(dim, (window_size, window_size), num_heads, qkv_bias, qk_scale, attn_drop, drop)\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        self.mlp = Mlp(dim, int(dim * mlp_ratio), act_layer, drop)\n",
    "\n",
    "        if self.shift_size > 0:\n",
    "            H, W = input_resolution\n",
    "            img_mask = torch.zeros((1, H, W, 1))\n",
    "            h_slices = (slice(0, -window_size), slice(-window_size, -shift_size), slice(-shift_size, None))\n",
    "            w_slices = (slice(0, -window_size), slice(-window_size, -shift_size), slice(-shift_size, None))\n",
    "            cnt = 0\n",
    "            for h in h_slices:\n",
    "                for w in w_slices:\n",
    "                    img_mask[:, h, w, :] = cnt\n",
    "                    cnt += 1\n",
    "            mask_windows = window_partition(img_mask, window_size)\n",
    "            mask_windows = mask_windows.view(-1, window_size * window_size)\n",
    "            attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
    "            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100)).masked_fill(attn_mask == 0, float(0))\n",
    "        else:\n",
    "            attn_mask = None\n",
    "\n",
    "        self.register_buffer(\"attn_mask\", attn_mask)\n",
    "\n",
    "    def forward(self, x):\n",
    "        H, W = self.input_resolution\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"Input feature has wrong size\"\n",
    "        shortcut = x\n",
    "        x = self.norm1(x).view(B, H, W, C)\n",
    "\n",
    "        if self.shift_size > 0:\n",
    "            x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n",
    "        x_windows = window_partition(x, self.window_size).view(-1, self.window_size * self.window_size, C)\n",
    "\n",
    "        attn_windows = self.attn(x_windows, mask=self.attn_mask)\n",
    "        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n",
    "        x = window_reverse(attn_windows, self.window_size, H, W)\n",
    "\n",
    "        if self.shift_size > 0:\n",
    "            x = torch.roll(x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n",
    "\n",
    "        x = x.view(B, H * W, C)\n",
    "        x = shortcut + x\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c54888",
   "metadata": {},
   "source": [
    "## ðŸ”» Patch Merging - `PatchMerging` Class\n",
    "\n",
    "This layer reduces the spatial resolution (like pooling) and increases the number of channels.  \n",
    "It's used between Swin Transformer stages to build a hierarchical representation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c65bea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchMerging(nn.Module):\n",
    "    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.input_resolution = input_resolution\n",
    "        self.dim = dim\n",
    "        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n",
    "        self.norm = norm_layer(4 * dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        H, W = self.input_resolution\n",
    "        B, L, C = x.shape\n",
    "        x = x.view(B, H, W, C)\n",
    "\n",
    "        x0 = x[:, 0::2, 0::2, :]\n",
    "        x1 = x[:, 0::2, 1::2, :]\n",
    "        x2 = x[:, 1::2, 0::2, :]\n",
    "        x3 = x[:, 1::2, 1::2, :]\n",
    "        x = torch.cat([x0, x1, x2, x3], -1).view(B, -1, 4 * C)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        x = self.reduction(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9e585d",
   "metadata": {},
   "source": [
    "## ðŸ§± Basic Layer - `BasicLayer` Class\n",
    "\n",
    "Each stage of the Swin Transformer consists of a sequence of `SwinTransformerBlock`s.  \n",
    "Optionally, a `PatchMerging` layer is added at the end of each stage (except the last one).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc5f18d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicLayer(nn.Module):\n",
    "    def __init__(self, dim, input_resolution, depth, num_heads, window_size=7, downsample=None):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList([\n",
    "            SwinTransformerBlock(dim, input_resolution, num_heads, window_size, shift_size=0 if i % 2 == 0 else window_size // 2)\n",
    "            for i in range(depth)\n",
    "        ])\n",
    "        self.downsample = downsample(input_resolution, dim) if downsample else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        if self.downsample:\n",
    "            x = self.downsample(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c49cb0d",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Patch Embedding - `PatchEmbed` Class\n",
    "\n",
    "This module splits the input image into non-overlapping patches using a convolutional layer and embeds them into a higher-dimensional space.  \n",
    "This is the Swin equivalent of tokenization in NLP.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "538bf9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=4, in_channels=3, embed_dim=96, norm_layer=None):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.norm = norm_layer(embed_dim) if norm_layer else None\n",
    "        self.grid_size = (img_size // patch_size, img_size // patch_size)\n",
    "        self.num_patches = self.grid_size[0] * self.grid_size[1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        if self.norm:\n",
    "            x = self.norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c5c8cb",
   "metadata": {},
   "source": [
    "##  Swin Transformer Backbone - `SwinTransformer` Class\n",
    "\n",
    "This is the main model class. It stacks several `BasicLayer`s with increasing depth and channel size.  \n",
    "Each layer processes the input features hierarchically, enabling both local and global feature extraction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d794566",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinTransformer(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=4, in_channels=3, embed_dim=96, depths=(2, 2, 6, 2), num_heads=8):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbed(img_size, patch_size, in_channels, embed_dim, norm_layer=nn.LayerNorm)\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, self.patch_embed.num_patches, embed_dim))\n",
    "        self.stages = nn.ModuleList()\n",
    "\n",
    "        for i in range(len(depths)):\n",
    "            dim = embed_dim * (2 ** i)\n",
    "            res = (self.patch_embed.grid_size[0] // (2 ** i), self.patch_embed.grid_size[1] // (2 ** i))\n",
    "            downsample = PatchMerging if i < len(depths) - 1 else None\n",
    "            self.stages.append(BasicLayer(dim, res, depths[i], num_heads, downsample=downsample))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        x = x + self.pos_embed\n",
    "        for stage in self.stages:\n",
    "            x = stage(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a8d6c7",
   "metadata": {},
   "source": [
    "## ðŸ§ª Testing the Model\n",
    "\n",
    "We run a forward pass on a dummy batch of images to verify that all components work together and to inspect the output shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15252aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 49, 768])\n"
     ]
    }
   ],
   "source": [
    "model = SwinTransformer()\n",
    "x = torch.randn(32, 3, 224, 224)  # Batch of 32 images\n",
    "out = model(x)\n",
    "print(out.shape)  # Expected output: (32, 49, 768)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
